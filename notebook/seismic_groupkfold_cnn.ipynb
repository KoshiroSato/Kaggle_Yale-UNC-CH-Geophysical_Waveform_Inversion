{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSagdEAg5gx0"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNBlfswH61tP"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "import csv\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from scipy.ndimage import gaussian_filter\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from pytorch_lightning.loggers import CSVLogger\n",
        "from pytorch_lightning import seed_everything\n",
        "from torchmetrics.image import StructuralSimilarityIndexMeasure as SSIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tSbllEx61nq"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    debug = False\n",
        "    use_l4_gpu = False\n",
        "    seed = 42\n",
        "    n_folds = 5\n",
        "    epochs = 100\n",
        "    train_batch_size = 64\n",
        "    val_batch_size = 64\n",
        "    test_batch_size = 8\n",
        "    num_workers = 4\n",
        "    es_patience = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTOwePjo61jG"
      },
      "outputs": [],
      "source": [
        "if Config.use_l4_gpu:\n",
        "    torch.set_float32_matmul_precision('high')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "va_cq_EN61fO"
      },
      "outputs": [],
      "source": [
        "seed_everything(Config.seed, workers=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OCSwWXauxO-D"
      },
      "outputs": [],
      "source": [
        "all_inputs = [\n",
        "    f for f in Path('../input/waveform-inversion/train_samples').rglob('*.npy')\n",
        "    if ('seis' in f.stem) or ('data' in f.stem)\n",
        "]\n",
        "\n",
        "def inputs_files_to_output_files(input_files):\n",
        "    return [Path(str(f).replace('seis', 'vel').replace('data', 'model')) for f in input_files]\n",
        "\n",
        "all_outputs = inputs_files_to_output_files(all_inputs)\n",
        "assert all(f.exists() for f in all_outputs)\n",
        "\n",
        "test_files = list(Path('../input/waveform-inversion/test').glob('*.npy'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LgvfW9dZrGN"
      },
      "outputs": [],
      "source": [
        "def apply_gaussian_filter(data, sigma=(1, 1)):\n",
        "    filtered = np.zeros_like(data)\n",
        "    for c in range(data.shape[0]):\n",
        "        filtered[c] = gaussian_filter(data[c], sigma=sigma)\n",
        "    return filtered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xK0tsw_r61Wv"
      },
      "outputs": [],
      "source": [
        "class SeismicDataset(Dataset):\n",
        "    def __init__(self, inputs_files, output_files, n_examples_per_file=500, augment=False):\n",
        "        assert len(inputs_files) == len(output_files)\n",
        "        self.inputs_files = inputs_files\n",
        "        self.output_files = output_files\n",
        "        self.n_examples_per_file = n_examples_per_file\n",
        "        self.augment = augment\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.inputs_files) * self.n_examples_per_file\n",
        "\n",
        "    def _apply_augmentation(self, x, y):\n",
        "        if np.random.rand() < 0.5:\n",
        "            x = x.copy()\n",
        "            y = y.copy()\n",
        "            noise = np.random.normal(0, np.random.uniform(0.001, 0.01), size=x.shape).astype(np.float32)\n",
        "            x += noise\n",
        "\n",
        "        if np.random.rand() < 0.5:\n",
        "            x = np.flip(x, axis=2).copy()\n",
        "            y = np.flip(y, axis=2 if y.ndim == 3 else 1).copy()\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        file_idx = idx // self.n_examples_per_file\n",
        "        sample_idx = idx % self.n_examples_per_file\n",
        "\n",
        "        X = np.load(self.inputs_files[file_idx], mmap_mode='r')\n",
        "        y = np.load(self.output_files[file_idx], mmap_mode='r')\n",
        "\n",
        "        x_sample = X[sample_idx].astype(np.float32).copy()\n",
        "        y_sample = y[sample_idx].astype(np.float32).copy()\n",
        "\n",
        "        if self.augment:\n",
        "            x_sample, y_sample = self._apply_augmentation(x_sample, y_sample)\n",
        "\n",
        "        x_sample = apply_gaussian_filter(x_sample)\n",
        "\n",
        "        return torch.from_numpy(x_sample), torch.from_numpy(y_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GMt9hWmQHND"
      },
      "outputs": [],
      "source": [
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, alpha=0.84, beta=0.16):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.L1Loss()\n",
        "        self.ssim = SSIM(data_range=1.0)\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        pred = (pred - 1500) / 1000\n",
        "        target = (target - 1500) / 1000\n",
        "\n",
        "        l1_loss = self.l1(pred, target)\n",
        "        ssim_loss = 1 - self.ssim(pred, target)\n",
        "\n",
        "        return self.alpha * l1_loss + self.beta * ssim_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBPT-Ohkr96h"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, use_se=True, dropout_prob=0.1):\n",
        "        super().__init__()\n",
        "        self.use_se = use_se\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "        self.dropout = nn.Dropout2d(dropout_prob) if dropout_prob > 0 else nn.Identity()\n",
        "\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1,\n",
        "                          stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels)\n",
        "            )\n",
        "        else:\n",
        "            self.downsample = None\n",
        "\n",
        "        if self.use_se:\n",
        "            self.se = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Conv2d(out_channels, out_channels // 16, kernel_size=1),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.Conv2d(out_channels // 16, out_channels, kernel_size=1),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        if self.use_se:\n",
        "            out = out * self.se(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        return self.relu(out)\n",
        "\n",
        "\n",
        "class SeismicModel(LightningModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        self.stem = nn.Sequential(\n",
        "            nn.Conv2d(5, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "        self.layer1 = nn.Sequential(\n",
        "            ResidualBlock(32, 64, stride=2, use_se=True, dropout_prob=0.1),\n",
        "            ResidualBlock(64, 64, stride=1, use_se=True, dropout_prob=0.1)\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResidualBlock(64, 128, stride=2, use_se=True, dropout_prob=0.1),\n",
        "            ResidualBlock(128, 128, stride=1, use_se=True, dropout_prob=0.1)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResidualBlock(128, 256, stride=2, use_se=True, dropout_prob=0.1),\n",
        "            ResidualBlock(256, 256, stride=1, use_se=True, dropout_prob=0.1)\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            ResidualBlock(256, 512, stride=2, use_se=True, dropout_prob=0.1),\n",
        "            ResidualBlock(512, 512, stride=1, use_se=True, dropout_prob=0.1)\n",
        "        )\n",
        "        self.layer5 = nn.Sequential(\n",
        "            ResidualBlock(512, 1024, stride=2, use_se=True, dropout_prob=0.1),\n",
        "            ResidualBlock(1024, 1024, stride=1, use_se=True, dropout_prob=0.1)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
        "        )\n",
        "\n",
        "        self.loss_fn = CombinedLoss()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.stem(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.layer5(x)\n",
        "        x = self.decoder(x)\n",
        "        x = F.interpolate(x, size=(70, 70), mode='bilinear', align_corners=False)\n",
        "        return x * 1000 + 1500\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        preds = self(x)\n",
        "        loss = self.loss_fn(preds, y)\n",
        "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        preds = self(x)\n",
        "        loss = self.loss_fn(preds, y)\n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True)\n",
        "        return loss\n",
        "\n",
        "    def predict_step(self, batch, batch_idx):\n",
        "        x, file_id = batch\n",
        "        return self(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters())\n",
        "        scheduler = {\n",
        "            'scheduler': ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True),\n",
        "            'monitor': 'val_loss',\n",
        "            'interval': 'epoch',\n",
        "            'frequency': 1\n",
        "        }\n",
        "        return {'optimizer': optimizer, 'lr_scheduler': scheduler}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a-gVit27SYU"
      },
      "outputs": [],
      "source": [
        "def run_train(fold, all_inputs, all_outputs, tr_idx, val_idx, config):\n",
        "    print(f'     ----- Fold: {fold} -----')\n",
        "    train_inputs = [all_inputs[i] for i in tr_idx]\n",
        "    val_inputs = [all_inputs[i] for i in val_idx]\n",
        "    if config.debug:\n",
        "        train_inputs = [train_inputs[0]]\n",
        "        val_inputs = [val_inputs[0]]\n",
        "    train_outputs = inputs_files_to_output_files(train_inputs)\n",
        "    val_outputs = inputs_files_to_output_files(val_inputs)\n",
        "\n",
        "    dstrain = SeismicDataset(train_inputs, train_outputs, augment=True)\n",
        "    dsvalid = SeismicDataset(val_inputs, val_outputs)\n",
        "\n",
        "    dltrain = DataLoader(dstrain,\n",
        "                         batch_size=config.train_batch_size,\n",
        "                         shuffle=True,\n",
        "                         num_workers=config.num_workers,\n",
        "                         pin_memory=True,\n",
        "                         drop_last=True,\n",
        "                         persistent_workers=True)\n",
        "\n",
        "    dlvalid = DataLoader(dsvalid,\n",
        "                         batch_size=config.val_batch_size,\n",
        "                         shuffle=False,\n",
        "                         num_workers=config.num_workers,\n",
        "                         pin_memory=True)\n",
        "\n",
        "    model = SeismicModel()\n",
        "\n",
        "    checkpoint = ModelCheckpoint(monitor='val_loss',\n",
        "                                 mode='min',\n",
        "                                 save_top_k=1,\n",
        "                                 save_weights_only=True,\n",
        "                                 dirpath=f'model_fold{fold}/')\n",
        "    es_callback = EarlyStopping(monitor='val_loss',\n",
        "                                patience=config.es_patience)\n",
        "    logger = CSVLogger(save_dir='logs/', name=f'model_fold{fold}')\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        max_epochs=config.epochs,\n",
        "        accelerator='auto',\n",
        "        logger=logger,\n",
        "        callbacks=[checkpoint,es_callback])\n",
        "\n",
        "    trainer.fit(model, dltrain, dlvalid)\n",
        "\n",
        "    model.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    mae_losses = []\n",
        "\n",
        "    for batch in dlvalid:\n",
        "        x, y = batch\n",
        "        x = x.to(model.device)\n",
        "        y = y.to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            preds = model(x)\n",
        "\n",
        "        preds_np = preds.cpu().numpy().reshape(preds.shape[0], -1)\n",
        "        y_np = y.cpu().numpy().reshape(y.shape[0], -1)\n",
        "\n",
        "        for pred_sample, y_sample in zip(preds_np, y_np):\n",
        "            mae = mean_absolute_error(y_sample, pred_sample)\n",
        "            mae_losses.append(mae)\n",
        "    mae_loss = np.mean(mae_losses)\n",
        "    print(f'Mean Absolute Error: {mae_loss:.5f}')\n",
        "\n",
        "    log_path = Path(logger.log_dir) / 'metrics.csv'\n",
        "    df = pd.read_csv(log_path)\n",
        "    df_train = df[['epoch', 'train_loss']].dropna()\n",
        "    df_val = df[['epoch', 'val_loss']].dropna()\n",
        "    df_merged = pd.merge(df_train, df_val, on='epoch')\n",
        "\n",
        "    plt.plot(df_merged['epoch'], df_merged['train_loss'], label='Train Loss')\n",
        "    plt.plot(df_merged['epoch'], df_merged['val_loss'], label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training & Validation Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return mae_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b08HzbI87SU1"
      },
      "outputs": [],
      "source": [
        "groups = [p.parts[4] for p in all_inputs]\n",
        "\n",
        "gkf = GroupKFold(n_splits=Config.n_folds)\n",
        "mae_scores = []\n",
        "for fold, (tr_idx, val_idx) in enumerate(gkf.split(all_inputs, groups=groups)):\n",
        "    mae_loss = run_train(fold, all_inputs, all_outputs, tr_idx, val_idx, Config)\n",
        "    mae_scores.append(mae_loss)\n",
        "print(f'     ----- {Config.n_folds}Folds Mean Absolute Error: {np.mean(mae_scores):.5f} -----')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1lhOYZPOxFW"
      },
      "outputs": [],
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, test_files):\n",
        "        self.test_files = test_files\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.test_files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        test_file = self.test_files[i]\n",
        "        x = np.load(test_file)\n",
        "        x = apply_gaussian_filter(x)\n",
        "        return torch.tensor(x, dtype=torch.float32), test_file.stem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5rb5oj5Ow7u"
      },
      "outputs": [],
      "source": [
        "ds = TestDataset(test_files)\n",
        "dl = DataLoader(ds, batch_size=Config.test_batch_size, num_workers=Config.num_workers, pin_memory=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IG5oStXfMuOh"
      },
      "outputs": [],
      "source": [
        "fold_preds = []\n",
        "for fold in range(Config.n_folds):\n",
        "    ckpt_path = glob.glob(f'model_fold{fold}/*.ckpt')[0]\n",
        "    model = SeismicModel.load_from_checkpoint(ckpt_path)\n",
        "    trainer = Trainer(accelerator='auto', devices=1 if torch.cuda.is_available() else None)\n",
        "    pred = trainer.predict(model, dl)\n",
        "    fold_preds.append(torch.cat(pred))\n",
        "    del model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "avg_preds = torch.stack(fold_preds).mean(dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twBSEVa9SLG-"
      },
      "outputs": [],
      "source": [
        "x_cols = [f'x_{i}' for i in range(1, 70, 2)]\n",
        "fieldnames = ['oid_ypos'] + x_cols\n",
        "\n",
        "with open('submission.csv', 'wt', newline='') as csvfile:\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "\n",
        "    for batch_preds, (_, oid_batch) in zip(avg_preds.split(Config.test_batch_size), dl):\n",
        "        y_preds = batch_preds[:, 0].numpy()\n",
        "\n",
        "        for y_pred, oid_test in zip(y_preds, oid_batch):\n",
        "            for y_pos in range(70):\n",
        "                row = dict(\n",
        "                    zip(\n",
        "                        x_cols,\n",
        "                        [y_pred[y_pos, x_pos] for x_pos in range(1, 70, 2)]\n",
        "                    )\n",
        "                )\n",
        "                row['oid_ypos'] = f\"{oid_test}_y_{y_pos}\"\n",
        "                writer.writerow(row)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}